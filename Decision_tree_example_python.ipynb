{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/earthianhivemind/DLlearning/blob/main/Decision_tree_example_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification Tree Example in Python\n",
        "\n",
        "We will run a simple example of a classification tree in Python with the aim of looking in more detail at the output we get from the scikit-learn package.\n",
        "\n",
        "We will be trying to predict whether a passenger survived the Titanic accident, based on their Age, Sex and the Class they were travelling on."
      ],
      "metadata": {
        "id": "vKuOIZkhgPV_"
      },
      "id": "vKuOIZkhgPV_"
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing and loading packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Set style for better-looking plots\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)"
      ],
      "metadata": {
        "id": "NxchMHF8fYD2"
      },
      "id": "NxchMHF8fYD2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Looking at the data"
      ],
      "metadata": {
        "id": "RjORSmt_ynyM"
      },
      "id": "RjORSmt_ynyM"
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the Titanic data\n",
        "# We'll create the aggregated dataset similar to the R version\n",
        "titanic_df = pd.DataFrame({\n",
        "    'Class': ['1st', '2nd', '3rd', 'Crew'] * 8,\n",
        "    'Sex': ['Male']*16 + ['Female']*16,\n",
        "    'Age': (['Child']*8 + ['Adult']*8) * 2,\n",
        "    'Survived': ['No', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes'] * 4,\n",
        "    'Freq': [0, 0, 35, 0, 5, 11, 13, 0,  # Male Child\n",
        "             118, 154, 387, 670, 57, 14, 75, 192,  # Male Adult\n",
        "             0, 0, 17, 0, 1, 13, 14, 0,  # Female Child\n",
        "             4, 13, 89, 3, 140, 80, 76, 20]  # Female Adult\n",
        "})\n",
        "\n",
        "print(titanic_df.head(10))\n",
        "print(\"\\nDataset summary:\")\n",
        "print(titanic_df.describe())\n",
        "print(\"\\nData types:\")\n",
        "print(titanic_df.dtypes)"
      ],
      "metadata": {
        "id": "BMyqtdijfkIm"
      },
      "id": "BMyqtdijfkIm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset shows the variables: Class (1st, 2nd, 3rd, and Crew), Sex (Male, Female), Age (Child, Adult), and Survived (No, Yes).\n",
        "\n",
        "Freq contains the number of cases observed for a particular combination of Class, Sex, Age and Survived."
      ],
      "metadata": {
        "id": "MybOWdS-lvO-"
      },
      "id": "MybOWdS-lvO-"
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert categorical variables to appropriate types\n",
        "titanic_df['Class'] = pd.Categorical(titanic_df['Class'], categories=['1st', '2nd', '3rd', 'Crew'], ordered=True)\n",
        "titanic_df['Sex'] = pd.Categorical(titanic_df['Sex'])\n",
        "titanic_df['Age'] = pd.Categorical(titanic_df['Age'])\n",
        "titanic_df['Survived'] = pd.Categorical(titanic_df['Survived'])\n",
        "\n",
        "print(titanic_df.info())"
      ],
      "metadata": {
        "id": "cat_convert"
      },
      "id": "cat_convert",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reshaping the data\n",
        "\n",
        "The Titanic dataset in R is aggregated: each row contains a frequency count representing multiple individuals.\n",
        "\n",
        "We need to expand this data so that we have one row per individual passenger. This is done by replicating rows according to their Freq values."
      ],
      "metadata": {
        "id": "reshape_header"
      },
      "id": "reshape_header"
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshaping of the data to one row per individual\n",
        "expanded = titanic_df.loc[titanic_df.index.repeat(titanic_df['Freq'])].reset_index(drop=True)\n",
        "expanded = expanded[['Class', 'Sex', 'Age', 'Survived']]\n",
        "\n",
        "print(f\"Expanded dataset shape: {expanded.shape}\")\n",
        "print(f\"Total passengers: {len(expanded)}\")\n",
        "print(\"\\nFirst 10 rows:\")\n",
        "print(expanded.head(10))"
      ],
      "metadata": {
        "id": "_CZF6T13HhCH"
      },
      "id": "_CZF6T13HhCH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can start looking at some descriptive statistics of the data. The plot below tells us about how different are the proportions of survivors by variable Sex. It is clear that Sex is a very strong predictor, as Females have a much larger chance at having survived compared to Males."
      ],
      "metadata": {
        "id": "dsCUZ0TAoB5P"
      },
      "id": "dsCUZ0TAoB5P"
    },
    {
      "cell_type": "code",
      "source": [
        "# Some descriptive statistics about the data\n",
        "# Sex\n",
        "sex_survival = pd.crosstab(expanded['Sex'], expanded['Survived'], normalize='index') * 100\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "sex_survival.plot(kind='bar', stacked=True, ax=ax, color=['#F8766D', '#00BFC4'])\n",
        "ax.set_ylabel('Proportion (%)')\n",
        "ax.set_xlabel('Sex')\n",
        "ax.set_title('Survival Proportion by Sex')\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
        "ax.legend(title='Survived', labels=['No', 'Yes'])\n",
        "\n",
        "# Add percentage labels on bars\n",
        "for container in ax.containers:\n",
        "    ax.bar_label(container, fmt='%.1f%%', label_type='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nSurvival rates by Sex:\")\n",
        "print(sex_survival)"
      ],
      "metadata": {
        "id": "L1OSzyeRkRHR"
      },
      "id": "L1OSzyeRkRHR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will finish by looking at the differences by Age. Although it is clearly a good predictor, notice that is less powerful at separating survivors from non-survivors than Sex."
      ],
      "metadata": {
        "id": "My9dwzroooan"
      },
      "id": "My9dwzroooan"
    },
    {
      "cell_type": "code",
      "source": [
        "# Some descriptive statistics about the data\n",
        "# Age\n",
        "age_survival = pd.crosstab(expanded['Age'], expanded['Survived'], normalize='index') * 100\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "age_survival.plot(kind='bar', stacked=True, ax=ax, color=['#F8766D', '#00BFC4'])\n",
        "ax.set_ylabel('Proportion (%)')\n",
        "ax.set_xlabel('Age')\n",
        "ax.set_title('Survival Proportion by Age')\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
        "ax.legend(title='Survived', labels=['No', 'Yes'])\n",
        "\n",
        "# Add percentage labels on bars\n",
        "for container in ax.containers:\n",
        "    ax.bar_label(container, fmt='%.1f%%', label_type='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nSurvival rates by Age:\")\n",
        "print(age_survival)"
      ],
      "metadata": {
        "id": "bV3qx1pnkhI3"
      },
      "id": "bV3qx1pnkhI3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Classification Tree\n",
        "\n",
        "Now we will build our classification tree model using scikit-learn's DecisionTreeClassifier."
      ],
      "metadata": {
        "id": "train_header"
      },
      "id": "train_header"
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the data for modeling\n",
        "# Convert categorical variables to numerical using one-hot encoding\n",
        "X = pd.get_dummies(expanded[['Class', 'Sex', 'Age']], drop_first=False)\n",
        "y = (expanded['Survived'] == 'Yes').astype(int)  # Convert to binary: 1 for Yes, 0 for No\n",
        "\n",
        "print(\"Feature columns:\")\n",
        "print(X.columns.tolist())\n",
        "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
        "print(f\"Target variable shape: {y.shape}\")"
      ],
      "metadata": {
        "id": "prep_data"
      },
      "id": "prep_data",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the classification tree\n",
        "# Using parameters similar to R's rpart defaults\n",
        "model = DecisionTreeClassifier(\n",
        "    criterion='gini',  # Similar to R's rpart default\n",
        "    random_state=42,\n",
        "    min_samples_split=20,  # Minimum samples to split a node\n",
        "    min_samples_leaf=7     # Minimum samples in a leaf\n",
        ")\n",
        "\n",
        "model.fit(X, y)\n",
        "\n",
        "print(\"Decision Tree Model trained successfully!\")\n",
        "print(f\"\\nTree depth: {model.get_depth()}\")\n",
        "print(f\"Number of leaves: {model.get_n_leaves()}\")\n",
        "print(f\"Number of features used: {model.n_features_in_}\")\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': model.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importance)"
      ],
      "metadata": {
        "id": "vYew5CfYdzOF"
      },
      "id": "vYew5CfYdzOF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions and accuracy\n",
        "predictions = model.predict(X)\n",
        "accuracy = (predictions == y).mean()\n",
        "\n",
        "print(f\"Training Accuracy: {accuracy:.2%}\")\n",
        "\n",
        "# Confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "cm = confusion_matrix(y, predictions)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(pd.DataFrame(cm,\n",
        "                   index=['Actual: No', 'Actual: Yes'],\n",
        "                   columns=['Predicted: No', 'Predicted: Yes']))\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y, predictions, target_names=['No', 'Yes']))"
      ],
      "metadata": {
        "id": "eval_model"
      },
      "id": "eval_model",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing the Decision Tree"
      ],
      "metadata": {
        "id": "viz_header"
      },
      "id": "viz_header"
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the tree with better visualization\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(model,\n",
        "          feature_names=X.columns,\n",
        "          class_names=['No', 'Yes'],\n",
        "          filled=True,\n",
        "          rounded=True,\n",
        "          fontsize=10)\n",
        "plt.title(\"Decision Tree: Titanic Survival Classification\", fontsize=16, pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "plot_tree"
      },
      "id": "plot_tree",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternative: Using graphviz for a cleaner tree visualization (if available)\n",
        "from sklearn.tree import export_text\n",
        "\n",
        "# Text representation of the tree\n",
        "tree_rules = export_text(model, feature_names=list(X.columns))\n",
        "print(\"Decision Tree Rules:\")\n",
        "print(tree_rules)"
      ],
      "metadata": {
        "id": "text_tree"
      },
      "id": "text_tree",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Looking at the tree plot\n",
        "\n",
        "* The root node (top box) shows the distribution before any splits take place. It displays the gini impurity, total samples, and the value counts for each class.\n",
        "\n",
        "* The first split indicates the most important variable. Here, Sex is the key predictor with males and females being separated.\n",
        "\n",
        "* Subsequent splits refine those decisions. For males, Age becomes important as children have higher survival rates. For females, Class becomes the distinguishing factor.\n",
        "\n",
        "* Looking at the tree allows us to see how the splitting happened. The boxes at the bottom (leaf nodes) show:\n",
        "  - The predicted class (No or Yes)\n",
        "  - The gini impurity (how pure the node is)\n",
        "  - The number of samples in that node\n",
        "  - The distribution of classes [No, Yes]\n",
        "\n",
        "* The color coding helps visualize the prediction: darker orange for \"No\" (didn't survive) and darker blue for \"Yes\" (survived).\n",
        "\n",
        "* The whole population is being split into groups based on combinations of Sex, Age, and Class.\n",
        "\n",
        "* Something you may notice is that the final nodes getting a prediction of \"Yes\" are typically more pure than those predicting \"No\". This means nodes predicting survival contain mostly survivors, whereas nodes predicting death contain a mix. This relates to the classification error, which we will explore in more detail later."
      ],
      "metadata": {
        "id": "X39OoDVCgDjT"
      },
      "id": "X39OoDVCgDjT"
    }
  ]
}